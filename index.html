<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>PromptKD</title>
  <link rel="icon" type="image/x-icon" href="none">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PromptKD: Unsupervised Prompt Distillation for Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <p> <a href="https://zhengli97.github.io/">Zheng Li</a><sup>1</sup>, <a href="https://implus.github.io/">Xiang Li</a> <sup>1*</sup>, Xinyi Fu<sup>2</sup>,
                  Xing Zhang<sup>1</sup>, Weiqiang Wang<sup>2</sup>, <a href="https://shuochenya.github.io/">Shuo Chen</a><sup>3</sup>, <a href="http://www.patternrecognition.asia/jian/">Jian Yang</a><sup>1*</sup>
                </p>
              </span>
              </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"> <sup>1</sup> VCIP, College of Computer Science, Nankai University,
                       <br> <sup>2</sup>Tiansuan Lab, Ant Group,  <sup>3</sup> RIKEN
                    <br>
                    <span class="author-block"><strong>CVPR 2024</strong></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
                    <br>
                    <span class="author-block"><small>zhengli97@mail.nankai.edu.com</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="  " target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                  
                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href=" " target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/zhengli97/PromptKD" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>

                  <!-- 中文解读 -->
                  <span class="link-block">
                    <a href="chinese_interpertation.html" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-clone"></i>
                    </span>
                    <span>中文解读</span>
                </a>
                <br><br><br>
                <figure>
                  <img src="static/images/framework2.png" alt="fail" width="100%"">
                  <figcaption class="content has-text-left"  style="word-break:normal">Figure 1. Architecture comparison between classic KD paradigm for CLIP (likewise CLIP-KD) and our PromptKD. 
                    <strong>(a)</strong> Classic KD methods perform distillation between independent teacher and student models. 
                    <strong>(b)</strong> PromptKD breaks the rules of teacher-student independence. 
                    We propose to reuse the previously well-trained text features from the teacher pre-training stage and incorporate them into the student image encoder for both distillation and inference.</figcaption>
                </figure>
                <br>
                <figure>
                  <img src="static/images/framework.png" alt="fail" width="100%"">
                  <figcaption class="content has-text-left"  style="word-break:normal">Figure 2. An overview of our PromptKD framework. 
                    <strong>(a)</strong> We first pre-train a large CLIP teacher model with labeled training images. 
                    <strong>(b)</strong> Reuse the existing higher-quality teacher text features for unsupervised prompt distillation.
                    <strong>(c)</strong> The well-trained student and pre-stored teacher text features are utilized for final inference.</figcaption>
                </figure>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p class="auto-wrap">
            Prompt learning has emerged as a valuable technique in enhancing vision-language models (VLMs) such as CLIP for downstream tasks in specific domains. 
            Existing work mainly focuses on designing various learning forms of prompts, neglecting the potential of prompts as effective distillers for learning from larger teacher models. 
            <br><br>
            In this paper, we introduce an unsupervised domain prompt distillation framework, which aims to transfer the knowledge of a larger teacher model to a lightweight target model through prompt-driven imitation using unlabeled domain images. 
            <br><br>
            Specifically, our framework consists of two distinct stages. In the initial stage, we pre-train a large CLIP teacher model using domain (few-shot) labels. 
            After pre-training, we leverage the unique decoupled-modality characteristics of CLIP by pre-computing and storing the text features as class vectors only once through the teacher text encoder.
            <br>
            In the subsequent stage, the stored class vectors are shared across teacher and student image encoders for calculating the predicted logits. 
            Further, we align the logits of both the teacher and student models via KL divergence loss, encouraging the student image encoder to generate similar probability distributions to the teacher through the learnable prompts.
            <br><br>
            The proposed prompt distillation process eliminates the reliance on labeled data, enabling the algorithm to leverage a vast amount of unlabeled images within the domain.
            Finally, the well-trained student image encoders and pre-stored text features (class vectors) are utilized for inference. 
            <br><br>
            To our best knowledge, we are the first to (1) perform unsupervised domain-specific prompt-driven knowledge distillation for CLIP, and (2) establish a practical pre-storing mechanism of text features as shared class vectors between teacher and student.
            Extensive experiments on 11 recognition datasets demonstrate the effectiveness of our method.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths" >
        <h2 class="title is 5">Highlights</h2>
        <div class="content has-text-justified" style="word-break:normal" >
          <p>
            (1). A novel two-stage unsupervised prompt distillation framework for Vision-Language Models.<p>
            (2). Reuse high-quality teacher text features instead of training the student's own text encoder. <p>
            (3). Distillation on large amounts of unlabeled domain images using soft labels provided by teacher.<p>
            (4). PromptKD outperforms all existing prompt learning methods on 11 diverse recognition datasets.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"">
        <h2 class="title is 5">Advantages</h2>
        <div class="content has-text-justified" style="word-break:normal" >
          <p>
            (1). We provide an efficient and simple CLIP distillation framework that can compress the knowledge of the ViT-L/14 CLIP model into the smaller ViT-B/16 CLIP model through prompt distillation.
            Notably, after distillation, the smaller ViT-B/16 CLIP model even achieves better performance than the vanilla ViT-L/14 CLIP on ImageNet-1K (77.62 HM vs. 76.52 HM).
            <p>
            (2). Due to the characteristics of CLIP, the text encoder only requires a single forward calculation for all classes.
            Inspired by this, we propose to reuse the existing higher-quality teacher text features instead of training the student's own text encoder.
             This approach not only maintains the quality of the text features but also significantly reduces computational costs and memory usage during training. 
            <p>
            (3). The existence of the teacher CLIP model liberates us from the need for labeled training samples. 
            Learning from large amounts of unlabeled domain images, which is easily accessible, allows the prompt to learn richer and more generalized domain representations.
            This significantly enhances student performance and makes our method easier to apply in real world scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths" style="width:100%">
        <h2 class="title is 5">Calculation Pipeline</h2>
        <div class="content has-text-left no-auto-width" style="word-break:normal" >
          <figure>
            <img src="static/images/pipeline.png" alt="fail" width="50%"">
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>x -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-left is-size-5">
        <strong>Base-to-Novel Experiments</strong>
        </div>
        <figure>
        <img src="static/images/exp_results.png" alt="fail" width="100%"">
        <figcaption class="content has-text-left" style="word-break:normal">Table 1. Comparison with existing state-of-the-art methods on base-to-novel generalization. 
          Our PromptKD demonstrates strong generalization ability and achieves significant improvements on 11 recognition datasets given the <strong> ViT-B/16 image encoder</strong> of the CLIP model.
          The symbol △ denotes the performance improvement compared to the previous SOTA method.
        </figure>
        <br>
        <figure>
          <img src="static/images/hm_score.png" alt="fail" width="60%"">
          <figcaption class="content has-text-centered" style="word-break:normal">Figure 3. Harmonic mean (HM) comparison on base-to-novel generalization.
        </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Cross Dataset Experiments</strong>
        </div>
        <figure>
          <img src="static/images/exp_results2.png" alt="fail" width="100%"">
          <figcaption class="content has-text-left" style="word-break:normal">
            Table 2. Comparison of PromptKD with existing advanced approaches on cross-dataset benchmark evaluation. 
            Based on our pipeline, we perform unsupervised prompt distillation using the unlabeled domain data respectively (i.e., the transductive setting). 
            The source model is trained on ImageNet. "ZSL" denotes the setting type for Zero-Shot Learning.
          </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Comparison with Other Methods</strong>
        </div>
        <figure>
          <img src="static/images/exp_results3.png" alt="fail" width="55%"">
          <figcaption class="content has-text-left" style="word-break:normal">
            Table 3. Comparison with existing works using unlabeled data on the Flowers102 dataset. Our method performs better than previous
            methods.
          </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Teacher Pre-training Methods</strong>
        </div>
        <figure>
          <img src="static/images/exp_results4.png" alt="fail" width="60%"">
          <figcaption class="content has-text-left" style="word-break:normal">
            Table 4. Comparison of different pre-training methods. Teacher
            pre-training with PromptSRC brings the best student performance. Notably, 
            any type of teacher model can enhance the student model with a non-trivial improvement.
          </figure>
          <br>
          <div class="content has-text-left is-size-5">
            <strong>Distillation with Different Teachers</strong>
          </div>
          <figure>
            <img src="static/images/exp_results5.png" alt="fail" width="50%"">
            <figcaption class="content has-text-left" style="word-break:normal">
              Figure 4. Comparison of distillation results for pre-trained teachers with different capacities. 
              Better teachers lead to better distillation performance.
            </figure>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content ">
    <h2 class="title">BibTeX</h2>
    <pre><code>BibTex Code Here</code></pre>
  </div>
</section> -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered" >
      <div class="column is-four-fifths" style="width:100%">
        <h2 class="title is 5">Video Demonstration</h2>
        <div class="content has-text-centered" style="word-break:normal" >
          <p>
            Video demonstration coming soon.</p>
        </div>
      </div>
    </div>
  </div>
</section>


<footer class="footer">
<div class="container is-small">
  <div class="columns is-centered ">
    <div class="column is-8">
      <div class="content is-small">
        <p>
          This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
        </p>
      </div>
    </div>
  </div>
</div>
</footer>


</body>
</html>
